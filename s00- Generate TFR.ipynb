{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1746f55-affe-458f-ab67-2fe81e4874e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile, pathlib\n",
    "os.environ[\"SEMI_PERSISTENT_DIRECTORY\"] = str(pathlib.Path(tempfile.gettempdir())/\"beam_scratch\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7c5d66-961d-4ca8-885d-416d38a1838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b53861c-ed30-4aeb-ba00-9be54d95c8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 11:55:54.194952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-28 11:55:54.222170: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-28 11:55:54.222225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO:absl:tensorflow_io is not available: No module named 'tensorflow_io'\n",
      "INFO:absl:tensorflow_ranking is not available: No module named 'tensorflow_ranking'\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:tensorflow_decision_forests is not available: No module named 'tensorflow_decision_forests'\n",
      "INFO:absl:struct2tensor is not available: No module named 'struct2tensor'\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_text is not available.\n",
      "INFO:absl:tensorflow_recommenders is not available.\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.Mapping[tensorflow_transform.beam.analyzer_cache.DatasetKey, typing.Mapping[str, tensorflow_transform.nodes.ValueNode]]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.Mapping[tensorflow_transform.beam.analyzer_cache.DatasetKey, tensorflow_transform.beam.analyzer_cache.DatasetCache]\n"
     ]
    }
   ],
   "source": [
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.orchestration import pipeline as tfx_pipeline\n",
    "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
    "from tfx.components import CsvExampleGen, StatisticsGen\n",
    "import os\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import tensorflow as tf\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf80def8-3c12-4f48-8668-944ef7767523",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tfx_spark_demo_parquet\"\n",
    "PARQUET_PATH = \"./tfx/data/parquet_demo/data.parquet\"\n",
    "TFRECORD_DIR = \"./tfx/data/parquet_tfr/\"\n",
    "BASE_DIR = os.path.abspath(\"./tfx/\")\n",
    "PIPELINE_ROOT = os.path.join(BASE_DIR, \"pipelines\", PIPELINE_NAME)\n",
    "METADATA_PATH = os.path.join(BASE_DIR, \"metadata\", PIPELINE_NAME, \"metadata.db\")\n",
    "SERVING_MODEL_DIR = os.path.join(BASE_DIR, \"serving_model\", PIPELINE_NAME)\n",
    "\n",
    "BEAM_ARGS = [\n",
    "    \"--runner=DirectRunner\",\n",
    "    \"--direct_running_mode=multi_threading\",\n",
    "    \"--direct_num_workers=2\",\n",
    "]\n",
    "os.makedirs(TFRECORD_DIR, exist_ok=True)\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(PIPELINE_ROOT, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n",
    "os.makedirs(SERVING_MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcca4d5f-e2a1-442f-85c3-c767711b5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_KEY = \"is_fraud\"          # e.g., \"is_fraud\" | set to None if you don't have one yet\n",
    "BINARY_CLASSIFICATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27382d31-bc5b-40d7-837c-ece6d3b8f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import pyarrow as pa\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5d68a5-d4fa-4d55-aae9-331d6cf68119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_feature_types(parquet_path):\n",
    "    d = ds.dataset(parquet_path, format=\"parquet\")\n",
    "    schema: pa.Schema = d.schema\n",
    "    types = {}\n",
    "    for f in schema:\n",
    "        t = f.type\n",
    "        if pa.types.is_integer(t):\n",
    "            types[f.name] = \"int\"\n",
    "        elif pa.types.is_floating(t) or pa.types.is_decimal(t):\n",
    "            types[f.name] = \"float\"\n",
    "        else:\n",
    "            types[f.name] = \"bytes\"\n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42f4afe-23c8-4a52-999c-fdf343941298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_tfexample(row, feature_types):\n",
    "    feats = {}\n",
    "    for k, v in row.items():\n",
    "        t = feature_types.get(k, \"bytes\")\n",
    "        if v is None:\n",
    "            if t == \"int\":\n",
    "                feats[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=[]))\n",
    "            elif t == \"float\":\n",
    "                feats[k] = tf.train.Feature(float_list=tf.train.FloatList(value=[]))\n",
    "            else:\n",
    "                feats[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[]))\n",
    "            continue\n",
    "\n",
    "        if not isinstance(v, (list, tuple)):\n",
    "            v = [v]\n",
    "\n",
    "        if t == \"int\":\n",
    "            v = [int(x) for x in v if x is not None]\n",
    "            feats[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n",
    "        elif t == \"float\":\n",
    "            v = [float(x) for x in v if x is not None]\n",
    "            feats[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v))\n",
    "        else:\n",
    "            v = [x if isinstance(x, (bytes, bytearray)) else str(x).encode(\"utf-8\") for x in v if x is not None]\n",
    "            feats[k] = tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feats)).SerializeToString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf5d0654-1131-466a-9d81-2dc42853fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadParquetAsDicts(beam.PTransform):\n",
    "    def __init__(self, file_pattern, columns=None):\n",
    "        super().__init__()\n",
    "        self.file_pattern = file_pattern\n",
    "        self.columns = columns\n",
    "\n",
    "    def expand(self, p):\n",
    "        from apache_beam.io.parquetio import ReadFromParquet\n",
    "        return p | \"ReadParquet\" >> ReadFromParquet(self.file_pattern, columns=self.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de59491-6040-4bc2-a899-ebe2d07a9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_to_tfrecords(parquet_path, output_dir, num_shards=32, beam_runner_args=None):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    feature_types = _infer_feature_types(parquet_path)\n",
    "    options = PipelineOptions(beam_runner_args or [])\n",
    "    prefix = os.path.join(output_dir, \"data\")\n",
    "    with beam.Pipeline(options=options) as p:\n",
    "        _ = (\n",
    "            p\n",
    "            | \"ReadParquetDicts\" >> ReadParquetAsDicts(parquet_path)\n",
    "            | \"RowToTFExample\" >> beam.Map(_to_tfexample, feature_types=feature_types)\n",
    "            | \"WriteTFRecords\" >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                file_path_prefix=prefix, file_name_suffix=\".tfrecord\", num_shards=num_shards\n",
    "            )\n",
    "        )\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b50cbd8-3e03-48ec-bbc6-b0ae7f5cf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT these paths\n",
    "parquet_path = \"./tfx/data/parquet_demo/data.parquet\"   # file or glob\n",
    "output_dir   = \"./tfx/data/parquet_tfr/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39d35de-8569-4e6f-955f-910a8402db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAM_ARGS = [\n",
    "    \"--runner=DirectRunner\",\n",
    "    \"--direct_running_mode=multi_threading\",\n",
    "    \"--direct_num_workers=2\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b2ebb7-f297-422b-9b8a-05bb96f28146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 39613\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 33073\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 44551\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 33393\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure control channel for localhost:39613.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Control channel established.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Initializing SDKHarness with unbounded number of workers.\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure control channel for localhost:39613.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Control channel established.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Initializing SDKHarness with unbounded number of workers.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure state channel for localhost:44551.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:State channel established.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure state channel for localhost:44551.\n",
      "INFO:apache_beam.runners.worker.data_plane:Creating client data channel for localhost:33073\n",
      "INFO:apache_beam.runners.worker.sdk_worker:State channel established.\n",
      "INFO:apache_beam.runners.worker.data_plane:Creating client data channel for localhost:33073\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 4 (skipped: 0), batches: 4, num_threads: 4\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 4 shards in 0.01 seconds.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:No more requests from control plane\n",
      "INFO:apache_beam.runners.worker.sdk_worker:SDK Harness waiting for in-flight requests to complete\n",
      "INFO:apache_beam.runners.worker.data_plane:Closing all cached grpc data channels.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Closing all cached gRPC state handlers.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Done consuming work.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:No more requests from control plane\n",
      "INFO:apache_beam.runners.worker.sdk_worker:SDK Harness waiting for in-flight requests to complete\n",
      "INFO:apache_beam.runners.worker.data_plane:Closing all cached grpc data channels.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Closing all cached gRPC state handlers.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Done consuming work.\n"
     ]
    }
   ],
   "source": [
    "prefix = parquet_to_tfrecords(\n",
    "    parquet_path=parquet_path,\n",
    "    output_dir=output_dir,\n",
    "    num_shards=4,\n",
    "    beam_runner_args=BEAM_ARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3489a298-e1d2-473b-836c-332866a3bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 39049\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 35409\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 46327\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 43229\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure control channel for localhost:39049.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Control channel established.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Initializing SDKHarness with unbounded number of workers.\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure control channel for localhost:39049.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Control channel established.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Initializing SDKHarness with unbounded number of workers.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure state channel for localhost:46327.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:State channel established.\n",
      "INFO:apache_beam.runners.worker.data_plane:Creating client data channel for localhost:35409\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Creating insecure state channel for localhost:46327.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:State channel established.\n",
      "INFO:apache_beam.runners.worker.data_plane:Creating client data channel for localhost:35409\n",
      "INFO:apache_beam.runners.worker.sdk_worker:No more requests from control plane\n",
      "INFO:apache_beam.runners.worker.sdk_worker:SDK Harness waiting for in-flight requests to complete\n",
      "INFO:apache_beam.runners.worker.data_plane:Closing all cached grpc data channels.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Closing all cached gRPC state handlers.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Done consuming work.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:No more requests from control plane\n",
      "INFO:apache_beam.runners.worker.sdk_worker:SDK Harness waiting for in-flight requests to complete\n",
      "INFO:apache_beam.runners.worker.data_plane:Closing all cached grpc data channels.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Closing all cached gRPC state handlers.\n",
      "INFO:apache_beam.runners.worker.sdk_worker:Done consuming work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "opts = PipelineOptions([\n",
    "    \"--runner=DirectRunner\",\n",
    "    \"--direct_running_mode=multi_threading\",\n",
    "    \"--direct_num_workers=2\",\n",
    "])\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    (\n",
    "        p\n",
    "        | \"Create\" >> beam.Create([1, 2, 3])\n",
    "        | \"Double\" >> beam.Map(lambda x: x * 2)\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4dbdc-ff11-4579-873c-47a7b72af293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFX Spark (py310)",
   "language": "python",
   "name": "tfx-spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
