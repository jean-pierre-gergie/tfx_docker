{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04bb6eb-73a3-4720-9610-d8253441c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 14:57:45.970273: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-19 14:57:45.983797: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-19 14:57:46.174804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-19 14:57:48.187795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lib versions:\n",
      "Python: 3.10.18 (main, Sep  8 2025, 21:42:39) [GCC 14.2.0]\n",
      "TF: 2.16.1\n",
      "TFDV: 1.16.1\n",
      "tfx: 1.16.0\n",
      "sklearn: 1.5.1\n",
      "pandas: 1.5.3\n",
      "onnx: 1.17.0\n",
      "onnxruntime: 1.22.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, glob, json, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType, StringTensorType, Int64TensorType\n",
    "import onnxruntime as ort\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Lib versions:\")\n",
    "import sys, tensorflow as tf, sklearn, onnx, tfx\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"TFDV:\", tfdv.__version__)\n",
    "print(\"tfx:\", tfx.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"onnx:\", onnx.__version__)\n",
    "print(\"onnxruntime:\", ort.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49ba684-f983-4146-897e-9c7c2eb1aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Parquet: /data/test_parquet_data.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
       "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
       "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
       "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
       "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0  M1979787155             0.0             0.0        0               0  \n",
       "1  M2044282225             0.0             0.0        0               0  \n",
       "2   C553264065             0.0             0.0        1               0  \n",
       "3    C38997010         21182.0             0.0        1               0  \n",
       "4  M1230701703             0.0             0.0        0               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"/data\"\n",
    "PARQUET_GLOB = os.path.join(DATA_DIR, \"**\", \"*.parquet\")\n",
    "parquet_files = sorted(glob.glob(PARQUET_GLOB, recursive=True))\n",
    "\n",
    "if parquet_files:\n",
    "    data_path = parquet_files[0]\n",
    "    print(f\"Found Parquet: {data_path}\")\n",
    "    # For very large files, you can load a sample. Here we try to read everything; adjust if needed.\n",
    "    df = pd.read_parquet(data_path)\n",
    "else:\n",
    "    print(\"No Parquet found in /data; generating a synthetic demo dataset.\")\n",
    "    rng = np.random.default_rng(7)\n",
    "    n = 5000\n",
    "    df = pd.DataFrame({\n",
    "        \"amount\": rng.normal(100, 30, size=n).round(2),\n",
    "        \"tx_count_30d\": rng.integers(0, 50, size=n),\n",
    "        \"category\": rng.choice([\"grocery\", \"fuel\", \"online\", \"travel\"], size=n, p=[0.5, 0.2, 0.2, 0.1]),\n",
    "        \"is_fraud\": rng.choice([0,1], size=n, p=[0.92, 0.08])\n",
    "    })\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15a4cd7-115f-4a35-9041-10b163b69937",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_200/4059008599.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"isFraud\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_200/4059008599.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"isFraud\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'target_column' is not defined"
     ]
    }
   ],
   "source": [
    "target_col = \"isFraud\"\n",
    "feature_cols = [c for c in df.columns if c not in target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6575e0a-edb3-468c-b9a9-5b3b36cd33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c0e7b-3735-4db4-ac09-03d16d668b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc972f-e659-4543-80cb-931f08b9e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(stats)\n",
    "print(\"Inferred schema with\", len(schema.feature), \"features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eebf7f-9337-410e-a3de-e029f10f9c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbcab9-4ad9-419a-b9ed-86c912a99c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.validate_statistics(statistics=stats, schema=schema)\n",
    "print(\"Anomalies found:\", anomalies.anomaly_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3710b-a741-4fa4-b064-8093d9c9dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tfdv.visualize_statistics(stats)\n",
    "    if len(anomalies.anomaly_info) > 0:\n",
    "        tfdv.display_anomalies(anomalies)\n",
    "except Exception as e:\n",
    "    print(\"Visualization skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38724733-7b42-445e-b3bb-64f4a0d0bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist schema for future runs\n",
    "os.makedirs(\"/metadata\", exist_ok=True)\n",
    "schema_path = \"/metadata/schema.pbtxt\"\n",
    "with open(schema_path, \"w\") as f:\n",
    "    f.write(str(schema))\n",
    "print(\"Schema saved to\", schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53efa2f6-8aa2-411b-abc8-9f2a86fbe22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y if y.nunique()==2 else None\n",
    ")\n",
    "\n",
    "# Preprocess: impute missing + scale numerics; impute + onehot categoricals\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[(\"preprocess\", preprocess), (\"rf\", clf)])\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = None\n",
    "try:\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "if y_proba is not None and y.nunique()==2:\n",
    "    try:\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "    except Exception as e:\n",
    "        print(\"ROC AUC unavailable:\", e)\n",
    "\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845c16e9-fb5e-43f1-a625-b4275cce8fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numeric_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_200/2860132697.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build ONNX input schema (initial_types) based on feature dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minitial_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minitial_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatTensorType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numeric_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Build ONNX input schema (initial_types) based on feature dtypes\n",
    "initial_types = []\n",
    "for c in numeric_cols:\n",
    "    initial_types.append((c, FloatTensorType([None, 1])))\n",
    "for c in categorical_cols:\n",
    "    # Use String for non-numeric; if integers with small cardinality are categorical, they were treated above\n",
    "    if pd.api.types.is_integer_dtype(df[c]):\n",
    "        initial_types.append((c, Int64TensorType([None, 1])))\n",
    "    else:\n",
    "        initial_types.append((c, StringTensorType([None, 1])))\n",
    "\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_types, target_opset=16)\n",
    "\n",
    "os.makedirs(\"/artifacts/onnx\", exist_ok=True)\n",
    "onnx_path = \"/artifacts/onnx/random_forest_pipeline.onnx\"\n",
    "with open(onnx_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"ONNX model saved to:\", onnx_path)\n",
    "\n",
    "# Quick sanity-check with onnxruntime\n",
    "sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Prepare a small batch from X_test in the same per-column dict format\n",
    "def to_onnx_inputs(df_batch):\n",
    "    feed = {}\n",
    "    for c in numeric_cols:\n",
    "        feed[c] = df_batch[[c]].astype(np.float32).values\n",
    "    for c in categorical_cols:\n",
    "        if pd.api.types.is_integer_dtype(df_batch[c]):\n",
    "            feed[c] = df_batch[[c]].astype(np.int64).values\n",
    "        else:\n",
    "            # onnxruntime expects bytes for string tensor\n",
    "            feed[c] = df_batch[[c]].astype(str).values\n",
    "    return feed\n",
    "\n",
    "sample = X_test.head(5).copy()\n",
    "inputs = to_onnx_inputs(sample)\n",
    "output_names = [o.name for o in sess.get_outputs()]\n",
    "preds = sess.run(output_names, inputs)\n",
    "\n",
    "print(\"ONNX outputs:\", {name: np.array(arr).shape for name, arr in zip(output_names, preds)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aefdc2-f784-47fa-9d08-e50b7dd04a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
